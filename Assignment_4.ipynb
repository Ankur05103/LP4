{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914abf6f-8c72-4fea-abcd-4dc1500a3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef6483-cd45-4e60-9b6d-cefc341414d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\ankur_vc1xnom\\\\Desktop\\\\LP4\\\\LP4\\\\ecg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833497ac-d179-4257-b94c-96fc3a97f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12cf53-b905-4974-924e-590f16b11e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [i for i in range(141)] \n",
    "df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f797a0be-926b-4687-8447-ca95fcbb4daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589339a2-902e-485c-b659-e87711297887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into features and target\n",
    "features = df.drop(140, axis=1)  # Features are all columns except the last (column 140)\n",
    "target = df[140]  # Target is the last column (column 140)\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2\n",
    ")\n",
    "\n",
    "# Get the indices of the training data points labeled as \"1\" (anomalies)\n",
    "train_index = y_train[y_train == 1].index\n",
    "\n",
    "# Select the training data points that are anomalies\n",
    "train_data = x_train.loc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d84c2b-804b-4183-b6f4-cfb8c44c8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Min-Max Scaler to scale the data between 0 and 1\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale the training data\n",
    "x_train_scaled = min_max_scaler.fit_transform(train_data.copy())\n",
    "\n",
    "# Scale the testing data using the same scaler\n",
    "x_test_scaled = min_max_scaler.transform(x_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c86214-0d5b-4b1b-979d-a3cd00839740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an Autoencoder model by extending the Model class from Keras\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "class AutoEncoder(Model):\n",
    "    def __init__(self, output_units, ldim=8):\n",
    "        super().__init__()\n",
    "        # Define the encoder part of the Autoencoder\n",
    "        self.encoder = Sequential([\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(ldim, activation='relu')\n",
    "        ])\n",
    "        # Define the decoder part of the Autoencoder\n",
    "        self.decoder = Sequential([\n",
    "            Dense(16, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.1),\n",
    "            Dense(output_units, activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass through the Autoencoder\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e558e01-e479-4cdb-83e9-2856a0415b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder(output_units=x_train_scaled.shape[1])\n",
    "\n",
    "model.compile(loss='msle', metrics=['mse'], optimizer='adam')\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_scaled,  \n",
    "    x_train_scaled,  \n",
    "    epochs=20,        \n",
    "    batch_size=512,   \n",
    "    validation_data=(x_test_scaled, x_test_scaled),  \n",
    "    shuffle=True     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be021d2f-f4bd-49c7-afe4-8b318b05eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee0e5e5-0886-4b51-afb8-f4a72174bc66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90c8166-9a31-4bc1-a52f-cc7e5b7d6475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_threshold(model, x_train_scaled):\n",
    "    recons = model.predict(x_train_scaled)\n",
    "\n",
    "    recons_error = tf.keras.metrics.msle(recons, x_train_scaled)\n",
    "\n",
    "    threshold = np.mean(recons_error.numpy()) + np.std(recons_error.numpy())\n",
    "\n",
    "    return threshold\n",
    "\n",
    "def get_predictions(model, x_test_scaled, threshold):\n",
    "    predictions = model.predict(x_test_scaled)\n",
    "\n",
    "    errors = tf.keras.losses.msle(predictions, x_test_scaled)\n",
    "\n",
    "    anomaly_mask = pd.Series(errors) > threshold\n",
    "\n",
    "    preds = anomaly_mask.map(lambda x: 0.0 if x == True else 1.0)\n",
    "\n",
    "    return preds\n",
    "\n",
    "threshold = find_threshold(model, x_train_scaled)\n",
    "print(f\"Threshold: {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016f72b-1116-4e1b-8f04-ecc514944ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "predictions = get_predictions(model, x_test_scaled, threshold)\n",
    "\n",
    "accuracy = metrics.accuracy_score(predictions, y_test)\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868deb9-6bfb-4180-93ba-2fd8cf48f646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b7af5-9962-4b4e-87ca-291f83803456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8db13c7-3e9e-4e31-bf01-a8dce881867e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
